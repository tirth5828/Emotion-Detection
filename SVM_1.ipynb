{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51419d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "#importing stopwords is optional, in this case it decreased accuracy\n",
    "#from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641cc73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\TIRTH\n",
      "[nltk_data]     JOSHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\TIRTH\n",
      "[nltk_data]     JOSHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\TIRTH\n",
      "[nltk_data]     JOSHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d215c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18439537",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb10d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"C:/Users/TIRTH JOSHI/Desktop/Sigmoid ML/dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cc3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.read_csv(\"C:/Users/TIRTH JOSHI/Desktop/Sigmoid ML/dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae531b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = train_data.columns.values[2:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a90ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love',\n",
       "       'optimism', 'pessimism', 'sadness', 'surprise', 'trust', 'neutral'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b13584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-21441</td>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-31535</td>\n",
       "      <td>Whatever you decide to do make sure it makes y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-21068</td>\n",
       "      <td>@Max_Kellerman  it also helps that the majorit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-31436</td>\n",
       "      <td>Accept the challenges so that you can literall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-En-22195</td>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7719</th>\n",
       "      <td>2018-En-01993</td>\n",
       "      <td>@BadHombreNPS @SecretaryPerry If this didn't m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>2018-En-01784</td>\n",
       "      <td>Excited to watch #stateoforigin tonight! Come ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>2018-En-04047</td>\n",
       "      <td>Blah blah blah Kyrie, IT, etc. @CJC9BOSS leavi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>2018-En-03041</td>\n",
       "      <td>#ThingsIveLearned The wise #shepherd never tru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7723</th>\n",
       "      <td>2018-En-03386</td>\n",
       "      <td>I am really flattered and happy to hear those ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7724 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  anger  \\\n",
       "0     2017-En-21441  “Worry is a down payment on a problem you may ...      0   \n",
       "1     2017-En-31535  Whatever you decide to do make sure it makes y...      0   \n",
       "2     2017-En-21068  @Max_Kellerman  it also helps that the majorit...      1   \n",
       "3     2017-En-31436  Accept the challenges so that you can literall...      0   \n",
       "4     2017-En-22195  My roommate: it's okay that we can't spell bec...      1   \n",
       "...             ...                                                ...    ...   \n",
       "7719  2018-En-01993  @BadHombreNPS @SecretaryPerry If this didn't m...      1   \n",
       "7720  2018-En-01784  Excited to watch #stateoforigin tonight! Come ...      0   \n",
       "7721  2018-En-04047  Blah blah blah Kyrie, IT, etc. @CJC9BOSS leavi...      1   \n",
       "7722  2018-En-03041  #ThingsIveLearned The wise #shepherd never tru...      0   \n",
       "7723  2018-En-03386  I am really flattered and happy to hear those ...      0   \n",
       "\n",
       "      anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
       "0                1        0     0    0     0         1          0        0   \n",
       "1                0        0     0    1     1         1          0        0   \n",
       "2                0        1     0    1     0         1          0        0   \n",
       "3                0        0     0    1     0         1          0        0   \n",
       "4                0        1     0    0     0         0          0        0   \n",
       "...            ...      ...   ...  ...   ...       ...        ...      ...   \n",
       "7719             0        1     0    0     0         0          0        0   \n",
       "7720             0        0     0    1     0         1          0        0   \n",
       "7721             0        1     0    0     0         0          0        1   \n",
       "7722             0        0     0    0     0         0          0        0   \n",
       "7723             0        0     0    1     0         1          0        0   \n",
       "\n",
       "      surprise  trust  neutral  \n",
       "0            0      1        0  \n",
       "1            0      0        0  \n",
       "2            0      0        0  \n",
       "3            0      0        0  \n",
       "4            0      0        0  \n",
       "...        ...    ...      ...  \n",
       "7719         0      0        0  \n",
       "7720         0      0        0  \n",
       "7721         0      0        0  \n",
       "7722         0      0        1  \n",
       "7723         0      0        0  \n",
       "\n",
       "[7724 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbceabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r\"http\\S+\", \"\", txt)\n",
    "    if len(txt) == 0:\n",
    "        return 'no text'\n",
    "    else:\n",
    "        txt = txt.split()\n",
    "        index = 0\n",
    "        for j in range(len(txt)):\n",
    "            if txt[j][0] == '@':\n",
    "                index = j\n",
    "        txt = np.delete(txt, index)\n",
    "        if len(txt) == 0:\n",
    "            return 'no text'\n",
    "        else:\n",
    "            words = txt[0]\n",
    "            for k in range(len(txt)-1):\n",
    "                words+= \" \" + txt[k+1]\n",
    "            txt = words\n",
    "            txt = re.sub(r'[^\\w]', ' ', txt)\n",
    "            if len(txt) == 0:\n",
    "                return 'no text'\n",
    "            else:\n",
    "                txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n",
    "                txt = txt.replace(\"'\", \"\")\n",
    "                txt = nltk.tokenize.word_tokenize(txt)\n",
    "                #data.content[i] = [w for w in data.content[i] if not w in stopset]\n",
    "                for j in range(len(txt)):\n",
    "                    txt[j] = lem.lemmatize(txt[j], \"v\")\n",
    "                if len(txt) == 0:\n",
    "                    return 'no text'\n",
    "                else:\n",
    "                    out = \"\"\n",
    "                    for i in txt:\n",
    "                        out += i + \" \"\n",
    "                    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f501f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Tweet'] = train_data['Tweet'].map(lambda x: cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59abcd21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       471\n",
      "           1       0.76      0.71      0.74       302\n",
      "\n",
      "    accuracy                           0.80       773\n",
      "   macro avg       0.79      0.78      0.79       773\n",
      "weighted avg       0.80      0.80      0.80       773\n",
      "\n",
      "anticipation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       670\n",
      "           1       0.27      0.14      0.18       103\n",
      "\n",
      "    accuracy                           0.84       773\n",
      "   macro avg       0.57      0.54      0.54       773\n",
      "weighted avg       0.80      0.84      0.81       773\n",
      "\n",
      "disgust\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       484\n",
      "           1       0.67      0.67      0.67       289\n",
      "\n",
      "    accuracy                           0.75       773\n",
      "   macro avg       0.74      0.74      0.74       773\n",
      "weighted avg       0.75      0.75      0.75       773\n",
      "\n",
      "fear\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       629\n",
      "           1       0.78      0.62      0.69       144\n",
      "\n",
      "    accuracy                           0.90       773\n",
      "   macro avg       0.85      0.79      0.82       773\n",
      "weighted avg       0.89      0.90      0.89       773\n",
      "\n",
      "joy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       491\n",
      "           1       0.75      0.68      0.71       282\n",
      "\n",
      "    accuracy                           0.80       773\n",
      "   macro avg       0.79      0.78      0.78       773\n",
      "weighted avg       0.80      0.80      0.80       773\n",
      "\n",
      "love\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       687\n",
      "           1       0.63      0.50      0.56        86\n",
      "\n",
      "    accuracy                           0.91       773\n",
      "   macro avg       0.79      0.73      0.75       773\n",
      "weighted avg       0.90      0.91      0.91       773\n",
      "\n",
      "optimism\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       556\n",
      "           1       0.65      0.65      0.65       217\n",
      "\n",
      "    accuracy                           0.80       773\n",
      "   macro avg       0.76      0.76      0.76       773\n",
      "weighted avg       0.80      0.80      0.80       773\n",
      "\n",
      "pessimism\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       679\n",
      "           1       0.35      0.18      0.24        94\n",
      "\n",
      "    accuracy                           0.86       773\n",
      "   macro avg       0.62      0.57      0.58       773\n",
      "weighted avg       0.83      0.86      0.84       773\n",
      "\n",
      "sadness\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84       538\n",
      "           1       0.65      0.58      0.61       235\n",
      "\n",
      "    accuracy                           0.78       773\n",
      "   macro avg       0.74      0.72      0.73       773\n",
      "weighted avg       0.77      0.78      0.77       773\n",
      "\n",
      "surprise\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       728\n",
      "           1       0.53      0.22      0.31        45\n",
      "\n",
      "    accuracy                           0.94       773\n",
      "   macro avg       0.74      0.60      0.64       773\n",
      "weighted avg       0.93      0.94      0.93       773\n",
      "\n",
      "trust\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       735\n",
      "           1       0.39      0.18      0.25        38\n",
      "\n",
      "    accuracy                           0.95       773\n",
      "   macro avg       0.67      0.58      0.61       773\n",
      "weighted avg       0.93      0.95      0.94       773\n",
      "\n",
      "neutral\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       758\n",
      "           1       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.97       773\n",
      "   macro avg       0.49      0.50      0.49       773\n",
      "weighted avg       0.96      0.97      0.97       773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "    lem = WordNetLemmatizer()\n",
    "    temp = train_data[[\"Tweet\",emotion]]\n",
    "    emotion_data = temp.copy()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(emotion_data.Tweet, emotion_data[emotion], test_size=0.1, random_state=0)\n",
    "    x_output = output_data[\"Tweet\"]\n",
    "    x_train = x_train.reset_index(drop = True)\n",
    "    x_test = x_test.reset_index(drop = True)\n",
    "    y_train = y_train.reset_index(drop = True)\n",
    "    y_test = y_test.reset_index(drop = True)\n",
    "    x_output = x_output.reset_index(drop = True)\n",
    "    vectorizer = TfidfVectorizer(min_df=3, max_df=0.9)\n",
    "    train_vectors = vectorizer.fit_transform(x_train)\n",
    "    test_vectors = vectorizer.transform(x_test)\n",
    "    output_vectors = vectorizer.transform(x_output)\n",
    "    model = svm.SVC(kernel='rbf',gamma='scale',probability = True,class_weight = 'balanced', degree = 8) \n",
    "    model.fit(train_vectors, y_train) \n",
    "    predicted_sentiment = model.predict(test_vectors)\n",
    "    output_sentiment = model.predict(output_vectors)\n",
    "    print(emotion)\n",
    "    print(classification_report(y_test, predicted_sentiment))\n",
    "    predicted_sentiments = []\n",
    "    for s in range(len(output_sentiment)):\n",
    "        predicted_sentiments.append(output_sentiment[s])\n",
    "    output_data[emotion] = predicted_sentiments\n",
    "#     prediction_df = pd.DataFrame({'Content':x_test,  emotion: predicted_sentiment})\n",
    "# prediction_df.to_csv('emotion_recognizer_svm.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60d2151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "filepath = Path('C:/Users/TIRTH JOSHI/Desktop/Sigmoid ML/dataset/output.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "output_data.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a12f2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = train_data[[\"Tweet\",\"anticipation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dedc53a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be a down payment on a problem you may never h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you decide to do make sure it make you happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it also help that the majority of NFL coach be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the challenge so that you can literally even f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roommate it s okay that we can t spell because...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7719</th>\n",
       "      <td>BadHombreNPS If this didn t make me so angry I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>to watch stateoforigin tonight Come on NSW ori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>Blah blah blah Kyrie IT etc leave Boston be th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>The wise shepherd never trust his flock to a s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7723</th>\n",
       "      <td>be really flatter and happy to hear those comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7724 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  anticipation\n",
       "0     be a down payment on a problem you may never h...             1\n",
       "1         you decide to do make sure it make you happy              0\n",
       "2     it also help that the majority of NFL coach be...             0\n",
       "3     the challenge so that you can literally even f...             0\n",
       "4     roommate it s okay that we can t spell because...             0\n",
       "...                                                 ...           ...\n",
       "7719  BadHombreNPS If this didn t make me so angry I...             0\n",
       "7720  to watch stateoforigin tonight Come on NSW ori...             0\n",
       "7721  Blah blah blah Kyrie IT etc leave Boston be th...             0\n",
       "7722  The wise shepherd never trust his flock to a s...             0\n",
       "7723  be really flatter and happy to hear those comp...             0\n",
       "\n",
       "[7724 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "191ad53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df2.Tweet, df2.anticipation, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d051de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reset_index(drop = True)\n",
    "x_test = x_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eedbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop = True)\n",
    "y_test = y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "276d5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62ca6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(x_train)\n",
    "test_vectors = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a13564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', degree=8) \n",
    "model.fit(train_vectors, y_train) \n",
    "predicted_sentiment = model.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb49bebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      1669\n",
      "           1       0.40      0.01      0.01       262\n",
      "\n",
      "    accuracy                           0.86      1931\n",
      "   macro avg       0.63      0.50      0.47      1931\n",
      "weighted avg       0.80      0.86      0.80      1931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b03e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = []\n",
    "for s in range(len(predicted_sentiment)):\n",
    "    predicted_sentiments.append(predicted_sentiment[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a0adff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame({'Content':x_test, 'Emotion_predicted':predicted_sentiment, 'Emotion_actual': y_test})\n",
    "prediction_df.to_csv('emotion_recognizer_svm.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
